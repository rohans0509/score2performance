{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_dir = \"Datasets/asap-dataset\"\n",
    "# read in json file as pandas dataframe\n",
    "annotations=pd.read_json(f\"{dataset_dir}/asap_annotations.json\").transpose()\n",
    "# add column for score_filename to annotations that converts row name to score_filename\n",
    "annotations['score_filename'] = annotations.index.map(lambda x: f\"{'/'.join(x.split('/')[:-1])}/midi_score.mid\")\n",
    "# rename index to performance_filename\n",
    "annotations.index=annotations.index.rename('performance_filename')\n",
    "annotations.reset_index(inplace=True)\n",
    "\n",
    "annotations.head(2)\n",
    "\n",
    "# only keep rows of annotations where score_and_performance_aligned is True\t\n",
    "annotations = annotations[annotations['score_and_performance_aligned'] == False]\n",
    "annotations.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing tokenised data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from midi_processing import mid2dat_anna\n",
    "import pickle\n",
    "\n",
    "store_folder=\"Store/asap-dataset\"\n",
    "\n",
    "# tokenise performances\n",
    "def tokenise_performances(annotations,dataset_dir=\"Datasets/asap-dataset\"):\n",
    "    token_dict={}\n",
    "    for index in tqdm(annotations.index):\n",
    "        row=annotations.loc[index]\n",
    "        # get performance filename\n",
    "        performance_filename = row['performance_filename']\n",
    "        # get performance file\n",
    "        performance_file = f\"{dataset_dir}/{performance_filename}\"\n",
    "        # tokenise performance\n",
    "        token_dict[performance_filename] = mid2dat_anna(performance_file)\n",
    "    return token_dict\n",
    "\n",
    "# tokenise scores\n",
    "def tokenise_scores(annotations,dataset_dir=\"Datasets/asap-dataset\"):\n",
    "    token_dict={}\n",
    "    # get unique score filenames\n",
    "    score_filenames = annotations['score_filename'].unique()\n",
    "    for score_filename in tqdm(score_filenames):\n",
    "        # get score filename\n",
    "        score_filename = score_filename\n",
    "        # get score file\n",
    "        score_file = f\"{dataset_dir}/{score_filename}\"\n",
    "        # tokenise score\n",
    "        token_dict[score_filename] = mid2dat_anna(score_file)\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise and save pickle of performances\n",
    "token_dict = tokenise_performances(annotations)\n",
    "with open(f'{store_folder}/perf_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(token_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from section import section\n",
    "from random import randint\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "store_folder=\"Store/asap-dataset\"\n",
    "dataset_dir = \"Datasets/asap-dataset\"\n",
    "\n",
    "# read in pickle of performances\n",
    "with open(f'{store_folder}/perf_dict.pickle', 'rb') as handle:\n",
    "    perf_dict = pickle.load(handle)\n",
    "# read in pickle of scores\n",
    "with open(f'{store_folder}/score_dict.pickle', 'rb') as handle:\n",
    "    score_dict = pickle.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import MidiToken\n",
    "from midi_processing import mid2dat_anna,dat2mid_anna\n",
    "\n",
    "# read annotations\n",
    "annotations=pd.read_json(f\"{dataset_dir}/asap_annotations.json\").transpose()\n",
    "# add column for score_filename to annotations that converts row name to score_filename\n",
    "annotations['score_filename'] = annotations.index.map(lambda x: f\"{'/'.join(x.split('/')[:-1])}/midi_score.mid\")\n",
    "# rename index to performance_filename\n",
    "annotations.index=annotations.index.rename('performance_filename')\n",
    "annotations.reset_index(inplace=True)\n",
    "# get unique performance filenames\n",
    "performance_filenames = annotations['performance_filename'].unique()\n",
    "\n",
    "# iterate over rows in annotations\n",
    "for index in tqdm(annotations.index):\n",
    "    # get row\n",
    "    row=annotations.loc[index]\n",
    "    # get performance filename\n",
    "    performance_filename = row['performance_filename']\n",
    "    # get performance file\n",
    "    performance_file = f\"{dataset_dir}/{performance_filename}\"\n",
    "    # get score filename\n",
    "    score_filename = row['score_filename']\n",
    "    # get score file\n",
    "    score_file = f\"{dataset_dir}/{score_filename}\"\n",
    "    ''' Get tokens'''\n",
    "    # get performance\n",
    "    performance_tokens = perf_dict[performance_filename]\n",
    "    # get score\n",
    "    score_tokens = score_dict[score_filename]\n",
    "    ''' Get beats'''\n",
    "    # get performance beats\n",
    "    performance_beats =  row['performance_beats']\n",
    "    # get score beats\n",
    "    score_beats = row['midi_score_beats']\n",
    "\n",
    "    start_beat=randint(0,len(performance_beats)-1)\n",
    "\n",
    "    performance_section,score_section=section(performance_tokens,performance_beats,score_tokens,score_beats)\n",
    "    \n",
    "    # add a SET_VELOCITY token to the beginning of the score section\n",
    "    score_section.insert(0,MidiToken(\"SET_VELOCITY\",8)) # Could skip\n",
    "\n",
    "    print(f\"Length of performance section: {len(performance_section)}\")\n",
    "\n",
    "    # write performance section to midi file\n",
    "    dat2mid_anna(performance_section,f\"p.mid\")\n",
    "\n",
    "    print(f\"Length of score section: {len(score_section)}\")\n",
    "    # write score section to midi file\n",
    "    dat2mid_anna(score_section,f\"s.mid\")\n",
    "\n",
    "    midi_image(f\"p.mid\")\n",
    "    midi_image(f\"s.mid\")\n",
    "\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Take a start beat from the performance and find the end beat which maximises the number of performance tokens\n",
    "2) Now we know the start beat and end beat, we can find the corresponding tokens in the score\n",
    "'''\n",
    "    \n",
    "# Save a dictionary {beat:token} for each performance and score\n",
    "def section(performance_tokens, performance_beats,score_tokens, scores_beats, start_beat=0):\n",
    "    start_beat=start_beat\n",
    "    end_beat=get_end_beat(performance_tokens, start_beat, performance_beats)\n",
    "    print(f\"start_beat: {start_beat}\")\n",
    "    print(f\"end_beat: {end_beat}\")\n",
    "\n",
    "    # For performance\n",
    "    start_token=beat2TokenPosition(start_beat, performance_beats, performance_tokens)\n",
    "    end_token=beat2TokenPosition(end_beat, performance_beats, performance_tokens)\n",
    "    # print(\"performance\")\n",
    "    # print(f\"start_token: {start_token}\")\n",
    "    # print(f\"end_token: {end_token}\")\n",
    "    performance_section=performance_tokens[start_token:end_token]\n",
    "\n",
    "    # For score\n",
    "    start_token=beat2TokenPosition(start_beat, scores_beats, score_tokens)\n",
    "    end_token=beat2TokenPosition(end_beat, scores_beats, score_tokens)\n",
    "    # print(\"score\")\n",
    "    # print(f\"start_token: {start_token}\")\n",
    "    # print(f\"end_token: {end_token}\")\n",
    "    score_section=score_tokens[start_token:end_token]\n",
    "\n",
    "    return performance_section, score_section\n",
    "\n",
    "\n",
    "\n",
    "def beat2TokenPosition(beat, beats, tokens):\n",
    "    beat_time=beats[beat]\n",
    "\n",
    "    time_shift_positions=[]\n",
    "\n",
    "    time_elapsed=0\n",
    "    # Iterate through performance_tokens, if the token is a time_shift token then add the time_shift to the time_elapsed\n",
    "    for i in range(len(tokens)):\n",
    "        token=tokens[i]\n",
    "        if token.type==\"TIME_SHIFT\":\n",
    "            time_elapsed+=token.value/1000\n",
    "            time_shift_positions.append((i, time_elapsed))\n",
    "    \n",
    "    # Iterate through time_shift_positions, if the time_elapsed is greater than the beat_time then return the index of the token\n",
    "    \n",
    "    for i in range(len(time_shift_positions)):\n",
    "        if time_shift_positions[i][1]>beat_time:\n",
    "            return time_shift_positions[i-1][0]\n",
    "    \n",
    "\n",
    "def get_end_beat(performance_tokens, start_beat, performance_beats,max_tokens=512):\n",
    "    end_beat=start_beat\n",
    "    \n",
    "    end_beat_token_position=beat2TokenPosition(end_beat, performance_beats, performance_tokens)\n",
    "    while end_beat_token_position<max_tokens:\n",
    "        end_beat+=1\n",
    "        end_beat_token_position=beat2TokenPosition(end_beat, performance_beats, performance_tokens)\n",
    "    return end_beat-1\n",
    "\n",
    "# a function that splits tokens into a list of list of tokens corresponding to beats\n",
    "\n",
    "def splitTokens(tokens,beats):\n",
    "    split_tokes=[]\n",
    "    # find tokens between consequtive beats\n",
    "    for i in range(len(beats)-1):\n",
    "        start_beat=beats[i]\n",
    "        end_beat=beats[i+1]\n",
    "        start_token=beat2TokenPosition(start_beat, beats, tokens)\n",
    "        end_token=beat2TokenPosition(end_beat, beats, tokens)\n",
    "        split_tokes.append(tokens[start_token:end_token])\n",
    "\n",
    "    return split_tokes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty midi read score_file\n",
    "from pretty_midi import PrettyMIDI\n",
    "midi_score = PrettyMIDI(score_file)\n",
    "\n",
    "start_beat=5\n",
    "position=beat2TokenPosition(start_beat,score_beats,score_tokens)\n",
    "print(position)\n",
    "print(score_tokens[position])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "Remember instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi as pm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function to midi image\n",
    "def midi_image(midi_file):\n",
    "    # read in midi file\n",
    "    midi_file = pm.PrettyMIDI(f\"{midi_file}\")\n",
    "    # get piano roll\n",
    "    piano_roll =  midi_file.get_piano_roll(fs=100) # shape=(pitch, timestep)\n",
    "    # plot the piano roll with length of yaxis=xaxis\n",
    "    \n",
    "    plt.imshow(piano_roll, aspect='auto', origin='lower')\n",
    "    plt.show()\n",
    "\n",
    "# plot first performance and corresponding score\n",
    "perf_index=0\n",
    "midi_image(f\"{dataset_dir}/{annotations.iloc[perf_index]['performance_filename']}\")\n",
    "midi_image(f\"{dataset_dir}/{annotations.iloc[perf_index]['score_filename']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
